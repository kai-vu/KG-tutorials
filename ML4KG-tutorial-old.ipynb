{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Knowledge Graphs and Semantic Technologies -- ML4KG Tutorial\n",
    "\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to install pykeen beforehand, see https://pykeen.readthedocs.io/en/stable/installation.html \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pykeen\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyKeen comes with its own datasets that can be used directly in a pipeline.\n",
    "Below we import it so that we can explore it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.datasets import Nations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want to be able tfo work with our own datasets as well, so we etch the online GoT dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pykeen import triples\n",
    "from pykeen.datasets.nations import NATIONS_TRAIN_PATH\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "\n",
    "# Format that can be read by a pd.from_csv should also be able to be read here, but the delimiter needs to be adjusted\n",
    "# PyKEEN uses tabs as defaults\n",
    "got = triples.TriplesFactory.from_path('GoT.csv',load_triples_kwargs=dict(delimiter=','))\n",
    "got_triples = got.triples\n",
    "got_triples[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "List the unique subject and object entities found in the dataset. Then list all of the relationships that link the entities (note that some entities are not linked). Create an RDF version of the dataset, using your own namespaces, and save is as a ttl file. \n",
    "\n",
    "Using SPARQL, answer the following questions : \n",
    "1. How many instances per class? Use ORDER BY to show the most popular class\n",
    "2. What is the most common relation per each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining train and test datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is typical in machine learning, we need to split our dataset into training and test (and sometimes validation) datasets.\n",
    "\n",
    "What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to ensure that all entities are represented in train and test sets by at least one triple.\n",
    "\n",
    "To accomplish this, PyKEEN provides the <b>pykeen.triples.TriplesFactory.split()</b> function, which defaults to an 80/20 split. It is also by default stratified, to ensure that the distribution of the test set corresponds to that of the training set. If you want to use early stopping, you will also need a validation set. The function takes a list of percentages as argument: if you want a 95/5 split you give it <b>[0.95,0.05]</b> as argument, if you want 90/5/5 (which would include a validation set as well) you give it <b>[0.9,0.05,0.05]</b> as argument and it will return 3 datasets.\n",
    "\n",
    "For sake of example, we will create a small test size that includes only 5% of triples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got_training, got_testing = got.split()\n",
    "got_training, got_testing = got.split([0.95,0.05])\n",
    "\n",
    "print('Train set size: ', got_training.triples.shape)\n",
    "print('Test set size: ', got_testing.triples.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create three train-test sets of different sizes from the GoT data. Give them different names. Make sure the test set is not too big when compared to the training set (test set should be max 15% of the total dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyKEEN has implemented several Knoweldge Graph Embedding models (TransE, ComplEx, DistMult, HolE, etc.). We will use the ComplEx model with default values for this tutorial.\n",
    "\n",
    "You can find the list of all implemented models in the documentation: https://pykeen.readthedocs.io/en/stable/reference/models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a model and instantiate it:\n",
    "There are two ways to import and use a model, both are shown below and don't give different results but not importing the model before hand might cause the automatic importing to be slower, especially if you plan to use the same model multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wee need the pipeline to run a model, so it is simpler to import it directly.\n",
    "# Pykeen lets you train a model with the minimal amount of custom parameters\n",
    "\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "# here we don't import the model, but let PyKEEN do the importing.\n",
    "pipeline_result_simple = pipeline(\n",
    "    random_seed=0,\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    ")\n",
    "pipeline_result_simple.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we import the model and use it directly.\n",
    "from pykeen.models import ComplEx\n",
    "\n",
    "pipeline_result_imported = pipeline(\n",
    "    random_seed=0,\n",
    "    model=ComplEx,\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    ")\n",
    "pipeline_result_imported.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve different metrics from the results. Here we retrieve the mean reciprocal rank (MRR). The result is the same for both the simple and imported model, because we used the same random seed (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_result_imported.get_metric('mrr'))\n",
    "print(pipeline_result_simple.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but to get a better performing model, you want to set different things\n",
    "pipeline_result = pipeline(\n",
    "    random_seed=0,\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    epochs=200,\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    ")\n",
    "print(pipeline_result.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the parameters:\n",
    "\n",
    "- dimensions : the dimensionality of the embedding space\n",
    "- negative_sampler : the negative samplic strategy, here set to default (not used in arguments).\n",
    "- batch_size : the number of batches in which the training set is split during the training loop. If you are having into low memory issues than settings this to a higher number may help.\n",
    "- epochs : the number of epochs to train the model for.\n",
    "- optimizer : the Adam optimizer, with a learning rate of $1e-3$ set via the <i>optimizer_kwarg</i>.\n",
    "- loss : pairwise loss, with a margin of $0.5$ set via the <i>loss_kwarg</i>.\n",
    "- regularizer :  regularization with $p=2$, i.e. $l_2$ regularization. $\\lambda$ = $1e-5$, set via the <i>regularizer_kwarg</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Negatives\n",
    "\n",
    "To ensure our model can be trained and evaluated correctly, we need to define a filter to ensure that no negative statements generated by the corruption procedure are actually positives. This is simply done by concatenating train and test sets. When negative triples are generated by the corruption strategy, we can check that they aren't actually true statements.\n",
    "\n",
    "With PyKEEN this is made very easy, and can simply be passed as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    epochs=200,\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    "    \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "    )\n",
    ")\n",
    "print(pipeline_result.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your learned model and also the results, we need to add checkpoints to the pipeline.\n",
    "By adding training kwargs to the pipeline, the model will be automatically saved. By default, it saves the model after every epoch (checkpoint_frequency=0). You can also set the directory to which the models are saved, but by default they will end up in ~/.data/pykeen/checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=200,\n",
    "        #checkpoint_name='got_complex_checkpoint.pt',\n",
    "        checkpoint_directory='checkpoint_dir/',\n",
    "        checkpoint_frequency=20,\n",
    "    ),\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to save models, but for that we need to do the training and evaluating outside of the pipeline model. Below is an example of the above model training outside of the pipeline module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.models import ComplEx\n",
    "model = ComplEx(triples_factory=got_training)\n",
    "\n",
    "from pykeen.optimizers import Adam\n",
    "optimizer = Adam(params=model.get_grad_params())\n",
    "\n",
    "# from pykeen.regularizers import LP\n",
    "# regularizer = LP(p=3,weight=1e-5)\n",
    "\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "training_loop = SLCWATrainingLoop(model=model,\n",
    "                                  triples_factory=got_training,\n",
    "                                  optimizer=optimizer)\n",
    "\n",
    "#training\n",
    "_ = training_loop.train(triples_factory=got_training,\n",
    "                    num_epochs=200)\n",
    "\n",
    "#evaluating\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "evaluator = RankBasedEvaluator()\n",
    "mapped_triples = got_testing.mapped_triples\n",
    "\n",
    "results = evaluator.evaluate(\n",
    "            model=model,\n",
    "            mapped_triples=mapped_triples,\n",
    "            )\n",
    "\n",
    "print(results.get_metric('mrr'))\n",
    "\n",
    "#save results, this works also with the pipeline results, as the results object \n",
    "#returned by the evaluator is the same as the one returned from the pipeline\n",
    "save_dir = 'got_complex'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "results.to_df().to_csv(save_dir+os.path.sep+'results.csv')\n",
    "\n",
    "import torch\n",
    "torch.save(model,'trained_model.pkl')\n",
    "\n",
    "#to load the model use the following command\n",
    "# my_pykeen_model = torch.load('trained_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Try changing the parameters of your training process. See if you obtain a better model in terms of average loss. Save it as ./data/best_model.pkl. Which parameters work best for the dataset? \n",
    "\n",
    "Now use the training and test set you created in Exercise 2. Which loss you obtain, and for which parameters? \n",
    "\n",
    "Remember to save each model locally with a different name, so you can find them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get some evaluation metrics for our model, they were already computed during evaluation time as part of the pipeline, and print them out.\n",
    "\n",
    "We are going to use the following evaluation metrics:\n",
    "- <i>mrr</i> (mean reciprocal rank) : this function computes the mean of the reciprocal of elements of a vector of rankings ranks\n",
    "- <i>hits_at_n</i> : this function computes how many elements of a vector of rankings ranks make it to the $top_n$ positions.\n",
    "\n",
    "NB : The choice of which _N_ makes more sense depends on the application and the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.get_metric('hits_at_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mrr = pipeline_result.get_metric('mrr')\n",
    "print(\"MRR: %.4f\" % (mrr))\n",
    "print()\n",
    "\n",
    "hits_10 = pipeline_result.get_metric('hits_at_10')\n",
    "print(\"Hits@10: %.6f\" % (hits_10))\n",
    "print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-10 better ranked triples.\\n\" % (hits_10*100))\n",
    "\n",
    "hits_3 = pipeline_result.get_metric('hits_at_3')\n",
    "print(\"Hits@3: %.6f\" % (hits_3))\n",
    "print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-3 better ranked triples.\\n\" % (hits_3*100))\n",
    "\n",
    "# hits_1 = hits_at_n_score(ranks, n=1)\n",
    "# print(\"Hits@1: %.2f\" % (hits_1))\n",
    "# print(\"Interpretation: on average, the model guessed the correct subject or object %.1f%% of the time when considering the top-1 better ranked triples.\\n\" % (hits_1*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Evaluate the models you created before (different set sizes, different parameters). Summarise your results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction allows to infer missing links in a graph. This has many real-world use cases, such as predicting connections between people in a social network, interactions between proteins in a biological network, and music recommendation based on prior user taste.\n",
    "\n",
    "In our case, we are going to see which of the following candidate statements is more likely to be true. Note that the candidate statements below are made up, i.e. they are not in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unseen = np.array([\n",
    "    ['Jorah Mormont', 'SPOUSE', 'Daenerys Targaryen'],\n",
    "    ['Tyrion Lannister', 'SPOUSE', 'Missandei'],\n",
    "    [\"King's Landing\", 'SEAT_OF', 'House Lannister of Casterly Rock'],\n",
    "    ['Sansa Stark', 'SPOUSE', 'Petyr Baelish'],\n",
    "    ['Daenerys Targaryen', 'SPOUSE', 'Jon Snow'],\n",
    "    ['Daenerys Targaryen', 'SPOUSE', 'Craster'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'The North'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'Dorne'],\n",
    "    ['House Tyrell of Highgarden', 'IN_REGION', 'Beyond the Wall'],\n",
    "    ['Brandon Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
    "    ['Brandon Stark', 'ALLIED_WITH', 'House Lannister of Casterly Rock'],    \n",
    "    ['Rhaegar Targaryen', 'PARENT_OF', 'Jon Snow'],\n",
    "    ['House Hutcheson', 'SWORN_TO', 'House Tyrell of Highgarden'],\n",
    "    ['Daenerys Targaryen', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
    "    ['Daenerys Targaryen', 'ALLIED_WITH', 'House Lannister of Casterly Rock'],\n",
    "    ['Jaime Lannister', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Robert I Baratheon', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Cersei Lannister', 'PARENT_OF', 'Myrcella Baratheon'],\n",
    "    ['Cersei Lannister', 'PARENT_OF', 'Brandon Stark'],\n",
    "    [\"Tywin Lannister\", 'PARENT_OF', 'Jaime Lannister'],\n",
    "    [\"Missandei\", 'SPOUSE', 'Grey Worm'],\n",
    "    [\"Brienne of Tarth\", 'SPOUSE', 'Jaime Lannister']\n",
    "])\n",
    "\n",
    "## we need to map the above triples to the id's which we used in our training/testing.\n",
    "## This information is stored in the triple factory \"got\", which we created at the beginning\n",
    "\n",
    "# unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))\n",
    "#     filter_triples=unseen_filter,   # Corruption strategy filter defined above \n",
    "#     corrupt_side = 's+o',\n",
    "#     use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
    "#     verbose=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pykeen import predict\n",
    "from pykeen.predict import predict_triples\n",
    "\n",
    "# got_unseen = triples.get_mapped_tripples(X_unseen,factory=got)\n",
    "pack = predict_triples(model=pipeline_result.model, triples=X_unseen, triples_factory=got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores are real numbers that need to be translated into probabilities [0,1] \n",
    "# for this, we use the expit transform.\n",
    "\n",
    "from scipy.special import expit\n",
    "processed_results = pack.process().df\n",
    "# print(processed_results)\n",
    "\n",
    "probs = expit(processed_results['score'])\n",
    "# print(probs)\n",
    "\n",
    "processed_results['prob'] = probs\n",
    "processed_results['triple'] = list(zip([' '.join(x) for x in X_unseen]))\n",
    "\n",
    "# processed_results\n",
    "pd.DataFrame(list(zip([' '.join(x) for x in X_unseen],  \n",
    "                      np.squeeze(processed_results['score']),\n",
    "                      np.squeeze(probs))), \n",
    "             columns=['statement', 'score', 'prob']).sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : the probabilities are not calibrated in any sense. To calibrate them, one may use a procedure such as [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling) or [Isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression). The challenge is to define what is a true triple and what is a false one, as the calibration of the probability of a triple being true depends on the base rate of positives and negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Analyse the results in the tables. Some predicted links are very likely to be true, others  capture things that never really happened. Can you spot which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Visualizing Embeddings\n",
    "\n",
    "It is possible to get an intuition on how the learned embedding are structured by plotting them into a 2-dimensional space. We can perform Principal Component Analysis on the entity embeddings, keeping 2 (or 3) principal components. ComplEx adds a further complication layer, as it uses complex vectors, which are in a sense already 2-dimensional. To tackle this, we will simply apply a Real-Valued Transformation by stacking the real and imaginary part of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 303212558.\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "Training epochs on cpu: 100%|██████████| 400/400 [01:26<00:00,  4.60epoch/s, loss=0.00218, prev_loss=0.00207]\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/159 [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 159/159 [00:00<00:00, 684triple/s] \n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.24s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09824293851852417\n",
      "0.1792452830188679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    epochs=400,\n",
    "    dimensions=12,\n",
    "    optimizer_kwargs={'lr':0.033},\n",
    "    loss = 'Negative Log Likelihood Loss',\n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':2, 'weight':1e-5}, \n",
    "    \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "        num_negs_per_pos=5\n",
    "    ),\n",
    "    evaluator_kwargs=dict(\n",
    "        filtered=True,\n",
    "    ),\n",
    "    training_kwargs={'batch_size': 5000}\n",
    ")\n",
    "print(pipeline_result.get_metric('mrr'))\n",
    "print(pipeline_result.get_metric('hits@10'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = pipeline_result.model\n",
    "entity_to_id = got_training.entity_to_id\n",
    "relation_to_id = got_training.relation_to_id\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "entity_embeddings = np.hstack([model.entity_representations[0](indices=None).detach().numpy().real, model.entity_representations[0](indices=None).detach().numpy().imag])\n",
    "m = pca.fit(entity_embeddings)\n",
    "\n",
    "eu = m.transform(entity_embeddings)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, n_init=10, random_state=42) #cheating a bit with the random state here\n",
    "cluster_labels = kmeans.fit_predict(eu)\n",
    "\n",
    "#let's pick some interesting names\n",
    "people = ['Eddard Stark', 'Arya Stark', 'Jon Snow',  \n",
    " 'Sansa Stark', 'Brandon Stark', 'Rickon Stark', 'Catelyn Stark', 'Robb Stark',  \n",
    " 'Benjen Stark', 'Lyanna Stark', 'Theon Greyjoy', 'Balon Greyjoy',  \n",
    " 'Daenerys Targaryen', 'Viserys Targaryen', 'Aegon Targaryen', 'Rhaegar Targaryen',  \n",
    " 'Tyrion Lannister', 'Jaime Lannister', 'Cersei Lannister', 'Tywin Lannister',  \n",
    " 'Kevan Lannister','Joffrey Baratheon', 'Tommen Baratheon',  \n",
    " 'Stannis Baratheon', 'Renly Baratheon', 'Robert I Baratheon',  \n",
    " 'Jorah Mormont', 'Jeor Mormont']\n",
    "houses = ['House Stark of Winterfell', 'House Lannister of Casterly Rock',\"House Targaryen of King's Landing\",\n",
    "  'House Baratheon of Dragonstone', 'House Tyrell of Highgarden', 'House Greyjoy of Pyke']  \n",
    "\n",
    "to_plot = people + houses\n",
    "ids = [entity_to_id[name] for name in to_plot]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sc = plt.scatter(eu[ids, 0], eu[ids, 1], c=cluster_labels[ids], cmap='tab10', alpha=0.7)\n",
    "\n",
    "for i, entity, c in zip(ids, people + houses, cluster_labels):\n",
    "    \n",
    "    plt.annotate(\n",
    "        text=entity,\n",
    "        xy=(eu[i, 0], eu[i, 1]),\n",
    "        color=\"tab:blue\",\n",
    "        ha=\"center\", va=\"top\"\n",
    "    )\n",
    "\n",
    "lim = 4\n",
    "\n",
    "\n",
    "plt.xlim([-lim, lim])\n",
    "plt.ylim([-lim, lim])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 More Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorboard](https://www.tensorflow.org/tensorboard) allows to dig into the workings of our model, plot how it is learning, and visualize [high-dimensional embeddings](https://projector.tensorflow.org/). See [this tutorial](https://www.tensorflow.org/tensorboard/get_started) to get started with Tensorflow and see [here](https://pykeen.readthedocs.io/en/stable/tutorial/trackers/using_tensorboard.html) for Tensorboard with PyKEEN.\n",
    "\n",
    "First ytou neeed to start the tensorboard web application from the command line with \n",
    "\n",
    "$ tensorboard --logdir=~/.data/pykeen/logs/tensorboard/\n",
    "\n",
    "and then we can add tensorboard as the result_tracker in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=got_training,\n",
    "    testing=got_testing,\n",
    "    training_kwargs=dict(\n",
    "        num_epochs=200\n",
    "    ),\n",
    "    dimensions=150,\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={'lr':1e-3},\n",
    "    loss='pairwisehinge', \n",
    "    regularizer='LP', \n",
    "    regularizer_kwargs={'p':3, 'weight':1e-5}, \n",
    "    negative_sampler='basic',\n",
    "    negative_sampler_kwargs=dict(\n",
    "        filtered=True,\n",
    "    ),\n",
    "    result_tracker='tensorboard'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 Your Own Data now\n",
    "\n",
    "Choose a dataset of your own. Best if it is the data you are using in your group project. \n",
    "\n",
    "- Create a training and testset. \n",
    "- Train your model to compute Knowledge Graph Embeddings, and save the best parameters model. - Predict new links over your dataset\n",
    "- Visualise the embeddings you computed \n",
    "- Optional : cluster your embeddings, [see this tutorial](https://docs.ampligraph.org/en/1.4.0/tutorials/ClusteringAndClassificationWithEmbeddings.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
